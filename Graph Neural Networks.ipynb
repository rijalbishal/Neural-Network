{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65edd5d2-1d95-4350-a611-2f2c613d6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')  # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c892a8-b06c-481e-ab68-9e832bc8793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Python libraries\n",
    "import urllib.request              # Used for downloading files from the internet (via HTTP requests)\n",
    "from urllib.error import HTTPError # Specific error class for handling download-related issues\n",
    "\n",
    "# Base URL where the pretrained model checkpoints are stored (GitHub raw files)\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
    "\n",
    "# List of pretrained model checkpoint files we want to download\n",
    "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
    "\n",
    "# Create a directory to store the downloaded checkpoint files if it doesn't already exist.\n",
    "# CHECKPOINT_PATH should be a variable (string) you defined earlier, e.g., \"./checkpoints/\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# Loop through each file in the pretrained_files list\n",
    "for file_name in pretrained_files:\n",
    "    # Construct the full local path where the file should be saved\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    \n",
    "    # If the file name includes a subdirectory (contains \"/\"), make sure that subdirectory exists\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\", 1)[0], exist_ok=True)\n",
    "    \n",
    "    # If the file is not already present locally, then download it\n",
    "    if not os.path.isfile(file_path):\n",
    "        # Create the full URL by combining the base_url with the specific file name\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")  # Inform the user which file is being downloaded\n",
    "        try:\n",
    "            # Download the file and save it to the specified local path\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            # If there's an error during download (e.g., file not found, no internet),\n",
    "            # print a helpful message along with the error details.\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, \"\n",
    "                  \"or contact the author with the full output including the following error:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66c0092-4305-4eee-811b-435ce1912f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Graph Convolutional Layer (GCNLayer) by subclassing nn.Module\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        \"\"\"\n",
    "        Initializes the GCN layer.\n",
    "\n",
    "        Args:\n",
    "            c_in  - Number of input features per node (feature dimensionality of input)\n",
    "            c_out - Number of output features per node (feature dimensionality of output)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Linear transformation to project node features from c_in -> c_out dimensions\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the GCN layer.\n",
    "\n",
    "        Inputs:\n",
    "            node_feats  - Tensor of node features with shape [batch_size, num_nodes, c_in].\n",
    "                          Each node has a feature vector of length c_in.\n",
    "            adj_matrix  - Tensor of adjacency matrices (graph structure) with shape\n",
    "                          [batch_size, num_nodes, num_nodes].\n",
    "                          If there is an edge from node i to j, adj_matrix[b, i, j] = 1.\n",
    "                          Identity/self-connections (diagonal ones) should already be added.\n",
    "\n",
    "        Output:\n",
    "            Updated node feature representations of shape [batch_size, num_nodes, c_out].\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute number of neighbors for each node\n",
    "        # This is used for normalization so each node's feature update is averaged\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)  # Shape: [batch_size, num_nodes, 1]\n",
    "\n",
    "        # Step 2: Apply a linear transformation (feature projection) to each node\n",
    "        node_feats = self.projection(node_feats)  # Shape: [batch_size, num_nodes, c_out]\n",
    "\n",
    "        # Step 3: Aggregate neighbor features using matrix multiplication\n",
    "        # torch.bmm = batch matrix multiplication\n",
    "        # adj_matrix [b, n, n] x node_feats [b, n, c_out] -> [b, n, c_out]\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "\n",
    "        # Step 4: Normalize by dividing by number of neighbors\n",
    "        # Ensures that the scale of features does not explode with more neighbors\n",
    "        node_feats = node_feats / num_neighbours\n",
    "\n",
    "        # Return the updated node features\n",
    "        return node_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400f0044-7457-414b-bbbd-d0bdae9860fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a toy example of node features for a graph with 4 nodes\n",
    "# torch.arange(8) -> generates values [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "# .view(1, 4, 2) -> reshapes into [batch_size=1, num_nodes=4, features_per_node=2]\n",
    "# So each node has 2 features, and we have 1 graph (batch size = 1)\n",
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "\n",
    "# Define an adjacency matrix for the same graph\n",
    "# Shape = [batch_size=1, num_nodes=4, num_nodes=4]\n",
    "# Row i corresponds to connections *from node i to other nodes*\n",
    "# A 1 means \"there is an edge\", 0 means \"no edge\"\n",
    "# The diagonal entries are 1 -> these are the self-loops (identity connections)\n",
    "adj_matrix = torch.Tensor([[\n",
    "    [1, 1, 0, 0],  # Node 0 is connected to itself and node 1\n",
    "    [1, 1, 1, 1],  # Node 1 is connected to all nodes (0,1,2,3)\n",
    "    [0, 1, 1, 1],  # Node 2 is connected to nodes 1,2,3\n",
    "    [0, 1, 1, 1]   # Node 3 is connected to nodes 1,2,3\n",
    "]])\n",
    "\n",
    "# Print the node features for clarity\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "\n",
    "# Print the adjacency matrix to visualize graph connections\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbfe4a7-82cd-49d3-8b59-aaeaad6efa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GCN layer\n",
    "# c_in=2 -> input feature dimension per node = 2\n",
    "# c_out=2 -> output feature dimension per node = 2\n",
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "\n",
    "# Manually set the weights of the linear projection to an identity matrix\n",
    "# [[1,0],[0,1]] means the projection won't change the input features\n",
    "# Bias is set to zero -> no shift in features\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "# Turn off gradient calculation since this is just a test (no training needed)\n",
    "with torch.no_grad():\n",
    "    # Pass the toy node features and adjacency matrix through the GCN layer\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "# Print inputs and outputs\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a13989-0bd6-4d25-9b3a-a7546a966bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc42493-0491-4858-bb95-2c4f1e614673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Graph Attention Layer\n",
    "# c_in = 2 -> input feature dimension per node = 2\n",
    "# c_out = 2 -> output feature dimension per node = 2\n",
    "# num_heads = 2 -> multi-head attention (the layer will compute attention twice in parallel)\n",
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "\n",
    "# Manually set the linear projection weights to identity\n",
    "# This ensures the projection does not change node features\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "# Manually set the attention parameters\n",
    "# Each head has its own attention vector `a` used in computing attention coefficients\n",
    "layer.a.data = torch.Tensor([\n",
    "    [-0.2, 0.3],   # Attention vector for head 1\n",
    "    [0.1, -0.1]    # Attention vector for head 2\n",
    "])\n",
    "\n",
    "# Run the GAT layer without gradients (we're just testing, not training)\n",
    "with torch.no_grad():\n",
    "    # Forward pass with attention printing enabled\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "# Print inputs and outputs for clarity\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b96e827-b839-47ed-a4dd-719c3a9a15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing PyTorch Geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # If it's not installed, we install the necessary packages.\n",
    "    # PyTorch Geometric has several dependencies that must match the PyTorch + CUDA version.\n",
    "\n",
    "    # Extract the installed PyTorch version (without \"+cuXXX\" suffix).\n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "\n",
    "    # Extract the CUDA version from torch (e.g., \"11.8\") and format as \"cu118\".\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
    "\n",
    "    # Install core PyTorch Geometric dependencies:\n",
    "    # - torch-scatter\n",
    "    # - torch-sparse\n",
    "    # - torch-cluster\n",
    "    # - torch-spline-conv\n",
    "    # These packages are provided from special wheels that match the current PyTorch+CUDA version.\n",
    "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "\n",
    "    # Finally, install the main torch-geometric package.\n",
    "    !pip install torch-geometric\n",
    "\n",
    "    # Import torch_geometric after installation\n",
    "    import torch_geometric\n",
    "\n",
    "# Import useful PyG submodules:\n",
    "import torch_geometric.nn as geom_nn     # Neural network layers (GCN, GAT, GraphSAGE, etc.)\n",
    "import torch_geometric.data as geom_data # Data handling (datasets, data loaders for graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2d799d-c5f0-4f6c-9f56-05b518939275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps string names to specific GNN layer classes from PyTorch Geometric\n",
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,        # Graph Convolutional Network (GCN) layer\n",
    "    \"GAT\": geom_nn.GATConv,        # Graph Attention Network (GAT) layer\n",
    "    \"GraphConv\": geom_nn.GraphConv # A more general graph convolution layer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ac28e3-f4fc-4241-8e99-9563f1eb2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cora citation network dataset using PyTorch Geometric's built-in Planetoid class\n",
    "cora_dataset = torch_geometric.datasets.Planetoid(\n",
    "    root=DATASET_PATH,  # Directory where the dataset should be stored (downloaded if not found)\n",
    "    name=\"Cora\"         # Name of the dataset to load (\"Cora\", \"CiteSeer\", \"PubMed\" are available)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95905a2f-06eb-4fc8-b371-b32706364189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930e9b7b-4244-4888-9d68-b8907bc8b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321934ad-8357-4f63-8ab2-c1f99bedc7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize a simple feedforward neural network (MLP).\n",
    "\n",
    "        Args:\n",
    "            c_in      - Number of input features (dimension of input vector per node)\n",
    "            c_hidden  - Size of hidden layers\n",
    "            c_out     - Number of output units (e.g., number of classes for classification)\n",
    "            num_layers- Number of layers (including input -> hidden layers). \n",
    "                        The last layer will always project to c_out.\n",
    "            dp_rate   - Dropout probability (to prevent overfitting)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []  # Collect all layers in a list\n",
    "\n",
    "        # Build (num_layers-1) hidden layers\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers - 1):\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),  # Fully connected layer\n",
    "                nn.ReLU(inplace=True),                 # Nonlinear activation\n",
    "                nn.Dropout(dp_rate)                    # Dropout for regularization\n",
    "            ]\n",
    "            in_channels = c_hidden  # Next layer input = hidden size\n",
    "\n",
    "        # Final output layer (maps hidden -> c_out, e.g., class scores)\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "\n",
    "        # Store the sequence of layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP.\n",
    "\n",
    "        Args:\n",
    "            x - Input features (tensor of shape [num_nodes, c_in] or [batch_size, c_in])\n",
    "        Returns:\n",
    "            Output tensor of shape [num_nodes, c_out] (e.g., class logits)\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f40e70-7315-4422-887d-9f42e6451c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save all hyperparameters (so Lightning can log them automatically)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Choose which model to use:\n",
    "        # - \"MLP\": a baseline without graph structure\n",
    "        # - otherwise: use a GNN model (e.g., GCN, GAT, GraphConv)\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "\n",
    "        # Define loss function for classification\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Forward pass through the model and compute loss/accuracy.\n",
    "\n",
    "        Args:\n",
    "            data - A PyG Data object containing:\n",
    "                   - x: node features\n",
    "                   - edge_index: graph structure\n",
    "                   - y: node labels\n",
    "                   - train/val/test masks\n",
    "            mode - Determines which mask to use (\"train\", \"val\", \"test\")\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)  # Run through MLP or GNN\n",
    "\n",
    "        # Select nodes for loss/accuracy based on mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        # Compute loss only on masked nodes\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "\n",
    "        # Compute accuracy: compare predicted class vs. true label\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer: Stochastic Gradient Descent with momentum + weight decay\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Compute training loss + accuracy\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)  # log for visualization\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Compute validation accuracy\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Compute test accuracy\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efbd3aff-ca5a-4a53-b141-df2f5a86251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         enable_progress_bar=False) # False because epoch size is 1\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything()\n",
    "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0dd51c3-2c68-4ea9-9213-702c9ebc46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a63da78a-46a1-4326-b515-ce2391d317d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelMLP.ckpt`\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train accuracy: 97.86%\n",
      "Val accuracy:   52.80%\n",
      "Test accuracy:  60.60%\n"
     ]
    }
   ],
   "source": [
    "# Train a node classification model on the Cora dataset\n",
    "node_mlp_model, node_mlp_result = train_node_classifier(\n",
    "    model_name=\"MLP\",       # Choose the model type (MLP baseline in this case)\n",
    "    dataset=cora_dataset,   # Dataset to use (Cora citation network)\n",
    "    c_hidden=16,            # Dimension of hidden layer features\n",
    "    num_layers=2,           # Number of layers in the MLP\n",
    "    dp_rate=0.1             # Dropout rate (for regularization)\n",
    ")\n",
    "\n",
    "# Print the results (accuracy, loss, etc.)\n",
    "print_results(node_mlp_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51fbb1aa-2c02-4bee-a34c-b870e9acd665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/opt/anaconda3/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelGNN.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train accuracy: 100.00%\n",
      "Val accuracy:   77.80%\n",
      "Test accuracy:  82.40%\n"
     ]
    }
   ],
   "source": [
    "# Train a node classification model on the Cora dataset using a GNN\n",
    "node_gnn_model, node_gnn_result = train_node_classifier(\n",
    "    model_name=\"GNN\",        # Specify we want a graph neural network (not MLP)\n",
    "    layer_name=\"GCN\",        # Choose GCNConv as the GNN layer type\n",
    "    dataset=cora_dataset,    # Dataset: Cora citation network\n",
    "    c_hidden=16,             # Hidden layer dimension\n",
    "    num_layers=2,            # Number of layers in the GNN\n",
    "    dp_rate=0.1              # Dropout rate\n",
    ")\n",
    "\n",
    "# Print evaluation results (train/val/test accuracy)\n",
    "print_results(node_gnn_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01aa1fb2-d17f-486b-a83d-f038493f0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MUTAG dataset using PyTorch Geometric's TUDataset class\n",
    "tu_dataset = torch_geometric.datasets.TUDataset(\n",
    "    root=DATASET_PATH,   # Directory where dataset should be stored (downloaded if missing)\n",
    "    name=\"MUTAG\"         # Dataset name from the TUDataset collection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23340a0c-9504-4e69-bf09-1fb685e2ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18dac7da-c077-4558-9677-76f2506ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tu_dataset.shuffle()\n",
    "train_dataset = tu_dataset[:150]\n",
    "test_dataset = tu_dataset[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9454c3ab-b32a-440d-8279-9eef44cb2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training dataset\n",
    "graph_train_loader = geom_data.DataLoader(\n",
    "    train_dataset,   # Training split of graphs\n",
    "    batch_size=64,   # Number of graphs per batch\n",
    "    shuffle=True     # Shuffle order each epoch (important for training)\n",
    ")\n",
    "\n",
    "# Create a DataLoader for validation dataset\n",
    "# (Here, it's reusing test_dataset as a placeholder, but usually you'd split off a validation set)\n",
    "graph_val_loader = geom_data.DataLoader(\n",
    "    test_dataset,    # Validation split of graphs\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Create a DataLoader for test dataset\n",
    "graph_test_loader = geom_data.DataLoader(\n",
    "    test_dataset,    # Test split of graphs\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99ed3fb4-f4af-4a64-839d-b8d39ff3ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Take the first batch from the test DataLoader\n",
    "batch = next(iter(graph_test_loader))\n",
    "\n",
    "# Print the whole batched object (will show sizes of x, edge_index, y, batch, etc.)\n",
    "print(\"Batch:\", batch)\n",
    "\n",
    "# Print the labels of the first 10 graphs in this batch\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "\n",
    "# Print the batch indices of the first 40 nodes\n",
    "# (This tells us which graph each node belongs to)\n",
    "print(\"Batch indices:\", batch.batch[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec91996-1797-41b5-87a5-9f2729bb8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a graph-level GNN model.\n",
    "\n",
    "        Args:\n",
    "            c_in            - Number of input features per node\n",
    "            c_hidden        - Hidden feature dimension inside the GNN layers\n",
    "            c_out           - Output feature dimension (number of graph classes)\n",
    "            dp_rate_linear  - Dropout probability before the final linear layer \n",
    "                              (usually higher than in the GNN to avoid overfitting)\n",
    "            kwargs          - Extra arguments passed to GNNModel (e.g., layer type, num_layers, dropout rate)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Core GNN backbone: maps node features -> hidden features\n",
    "        # Output size is c_hidden (not the final prediction yet)\n",
    "        self.GNN = GNNModel(\n",
    "            c_in=c_in,\n",
    "            c_hidden=c_hidden,\n",
    "            c_out=c_hidden,  # intermediate features\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Classification head: turns pooled graph representation into class logits\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),   # Regularization\n",
    "            nn.Linear(c_hidden, c_out)    # Final classifier (graph-level output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Forward pass for graph classification.\n",
    "\n",
    "        Args:\n",
    "            x         - Node features tensor [num_nodes, c_in]\n",
    "            edge_index- Graph connectivity (PyG COO format) [2, num_edges]\n",
    "            batch_idx - Batch vector mapping each node to its graph index\n",
    "                        (e.g., [0,0,0,1,1,1,...] means first 3 nodes in graph 0, next 3 in graph 1, etc.)\n",
    "\n",
    "        Returns:\n",
    "            Graph-level predictions (logits) of shape [num_graphs, c_out]\n",
    "        \"\"\"\n",
    "        # Step 1: Apply GNN layers (node-level feature updates)\n",
    "        x = self.GNN(x, edge_index)  # Shape: [num_nodes, c_hidden]\n",
    "\n",
    "        # Step 2: Pool node embeddings into graph embeddings\n",
    "        # global_mean_pool: averages node features for each graph in the batch\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx)  # Shape: [num_graphs, c_hidden]\n",
    "\n",
    "        # Step 3: Apply classification head to get logits\n",
    "        x = self.head(x)  # Shape: [num_graphs, c_out]\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d896d660-6592-4169-ab9c-08f8a4469cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save all hyperparameters (for logging/reproducibility in Lightning)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Core GNN model (node → pooled graph embedding → classification head)\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "\n",
    "        # Choose appropriate loss function:\n",
    "        # - BCEWithLogitsLoss for binary classification (c_out == 1)\n",
    "        # - CrossEntropyLoss for multi-class classification (c_out > 1)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Forward pass for graph classification.\n",
    "\n",
    "        Args:\n",
    "            data - A batch of graphs from DataLoader with attributes:\n",
    "                   - x: node features\n",
    "                   - edge_index: graph edges\n",
    "                   - batch: node-to-graph assignment\n",
    "                   - y: graph labels\n",
    "            mode - (Optional) \"train\" / \"val\" / \"test\" for logging\n",
    "\n",
    "        Returns:\n",
    "            loss, acc - loss value and accuracy metric\n",
    "        \"\"\"\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Run through GNN backbone + pooling + classification head\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "\n",
    "        # For binary classification, remove trailing dimension\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        # Convert logits into predictions\n",
    "        if self.hparams.c_out == 1:\n",
    "            # Binary classification → threshold at 0\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()  # Ensure labels are float for BCE loss\n",
    "        else:\n",
    "            # Multi-class classification → argmax over logits\n",
    "            preds = x.argmax(dim=-1)\n",
    "\n",
    "        # Compute loss (logits vs labels)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "\n",
    "        # Compute accuracy (predictions vs true labels)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW optimizer (good default for GNNs)\n",
    "        # High learning rate because MUTAG is a small dataset with a small model\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Run forward pass in training mode\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        # Log training loss + accuracy\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Evaluate validation accuracy\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Evaluate test accuracy\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21cfac93-0c0c-43d6-b502-fe3783739a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    # Set random seed for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Define a checkpoint directory for this model\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    # Create PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,   # Where logs/checkpoints are stored\n",
    "        callbacks=[ModelCheckpoint(  # Save only the best model (based on val_acc)\n",
    "            save_weights_only=True,\n",
    "            mode=\"max\",\n",
    "            monitor=\"val_acc\"\n",
    "        )],\n",
    "        accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",  # Auto GPU/CPU\n",
    "        devices=1,                 # Use 1 device\n",
    "        max_epochs=500,            # Train for max 500 epochs\n",
    "        enable_progress_bar=False  # Disable progress bar (cleaner output)\n",
    "    )\n",
    "\n",
    "    # Disable default \"hp_metric\" logging (not needed here)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check if a pretrained checkpoint already exists\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        # Load model directly from checkpoint\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        # Otherwise, initialize and train a new model\n",
    "        pl.seed_everything(42)  # Re-set seed to ensure reproducibility\n",
    "        model = GraphLevelGNN(\n",
    "            c_in=tu_dataset.num_node_features,                    # Input size = node feature dim\n",
    "            c_out=1 if tu_dataset.num_classes == 2 else tu_dataset.num_classes,  # Output = 1 (binary) or num_classes\n",
    "            **model_kwargs\n",
    "        )\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)   # Train model\n",
    "        # Reload the best model (based on validation accuracy)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Evaluate the best model on train and test sets\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "\n",
    "    # Collect results into a dictionary\n",
    "    result = {\n",
    "        \"test\": test_result[0]['test_acc'],\n",
    "        \"train\": train_result[0]['test_acc']\n",
    "    }\n",
    "\n",
    "    return model, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f8c7f4-c8b0-4701-b1e9-0cc3b9640acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/GraphLevelGraphConv.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:484: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
    "                                       c_hidden=256,\n",
    "                                       layer_name=\"GraphConv\",\n",
    "                                       num_layers=3,\n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e1d07c-3065-49ea-9713-fec051db66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 93.28%\n",
      "Test performance:  92.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
