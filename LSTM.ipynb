{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6237defa-f1d2-4d21-be55-a0354537d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports #####\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347f5ed7-1ca4-4b56-b5ff-66012e561310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 866, Char Size: 32\n"
     ]
    }
   ],
   "source": [
    "##### Data #####\n",
    "# Define input text (Hamlet excerpt, converted to lowercase)\n",
    "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
    "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
    "trageous fortune, Or to take arms against a sea of troubles A\\\n",
    "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
    "leep to say we end The heart-ache and the thousand natural sh\\\n",
    "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
    "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
    "there's the rub: For in that sleep of death what dreams may c\\\n",
    "ome, When we have shuffled off this mortal coil, Must give us\\\n",
    " pause—there's the respect That makes calamity of so long lif\\\n",
    "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
    "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
    "d love, the law's delay, The insolence of office, and the spu\\\n",
    "rns That patient merit of th'unworthy takes, When he himself \\\n",
    "might his quietus make\"\"\".lower()  # convert to lowercase for simplicity\n",
    "\n",
    "# Create set of unique characters in the dataset\n",
    "chars = set(data)\n",
    "\n",
    "# Data size = number of characters in text\n",
    "# Char size = number of unique characters (vocabulary size)\n",
    "data_size, char_size = len(data), len(chars)\n",
    "print(f'Data size: {data_size}, Char Size: {char_size}')\n",
    "\n",
    "# Create mapping: character → index\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "# Create mapping: index → character\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Training data:\n",
    "# train_X = all chars except last\n",
    "# train_y = all chars except first (shifted by one position)\n",
    "train_X, train_y = data[:-1], data[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40a67ed-1b87-4540-8135-6c5bf7dc4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions #####\n",
    "\n",
    "def oneHotEncode(text):\n",
    "    # Create a zero vector of shape (char_size, 1)\n",
    "    # char_size = number of unique characters in dataset\n",
    "    output = np.zeros((char_size, 1))\n",
    "    \n",
    "    # Set the index corresponding to the given character = 1\n",
    "    output[char_to_idx[text]] = 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Xavier Normalized Initialization for weight matrices\n",
    "def initWeights(input_size, output_size):\n",
    "    # Random uniform initialization in range [-1, 1]\n",
    "    # Scaled by sqrt(6 / (input_size + output_size)) for Xavier normalization\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2ee9e9-754b-472f-820c-b48a6eca0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Activation Functions #####\n",
    "\n",
    "def sigmoid(input, derivative=False):\n",
    "    # Sigmoid activation: maps values to range (0, 1)\n",
    "    # If derivative=True → return gradient (used in backprop)\n",
    "    if derivative:\n",
    "        return input * (1 - input)\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "\n",
    "def tanh(input, derivative=False):\n",
    "    # Tanh activation: maps values to range (-1, 1)\n",
    "    # If derivative=True → return gradient (1 - input^2)\n",
    "    if derivative:\n",
    "        return 1 - input ** 2\n",
    "    return np.tanh(input)\n",
    "\n",
    "\n",
    "def softmax(input):\n",
    "    # Softmax activation: converts raw scores into probabilities\n",
    "    # Each value in output ∈ (0,1) and all sum to 1\n",
    "    return np.exp(input) / np.sum(np.exp(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1704bc1f-7fc6-40f0-83d3-f93f34338cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Long Short-Term Memory Network Class #####\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate weights & bias\n",
    "        self.wf = initWeights(input_size, hidden_size)\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate weights & bias\n",
    "        self.wi = initWeights(input_size, hidden_size)\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate weights & bias\n",
    "        self.wc = initWeights(input_size, hidden_size)\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate weights & bias\n",
    "        self.wo = initWeights(input_size, hidden_size)\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final output layer weights & bias\n",
    "        self.wy = initWeights(hidden_size, output_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    # Reset network memory (hidden, cell states, and gates)\n",
    "    def reset(self):\n",
    "        self.concat_inputs = {}\n",
    "\n",
    "        self.hidden_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.activation_outputs = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, inputs):\n",
    "        self.reset()   # reset memory before each forward pass\n",
    "        outputs = []\n",
    "\n",
    "        for q in range(len(inputs)):\n",
    "            # Concatenate hidden state and input vector\n",
    "            self.concat_inputs[q] = np.concatenate((self.hidden_states[q - 1], inputs[q]))\n",
    "\n",
    "            # Compute gate activations\n",
    "            self.forget_gates[q]   = sigmoid(np.dot(self.wf, self.concat_inputs[q]) + self.bf)\n",
    "            self.input_gates[q]    = sigmoid(np.dot(self.wi, self.concat_inputs[q]) + self.bi)\n",
    "            self.candidate_gates[q]= tanh(np.dot(self.wc, self.concat_inputs[q]) + self.bc)\n",
    "            self.output_gates[q]   = sigmoid(np.dot(self.wo, self.concat_inputs[q]) + self.bo)\n",
    "\n",
    "            # Update cell state\n",
    "            self.cell_states[q] = self.forget_gates[q] * self.cell_states[q - 1] + \\\n",
    "                                  self.input_gates[q] * self.candidate_gates[q]\n",
    "\n",
    "            # Update hidden state\n",
    "            self.hidden_states[q] = self.output_gates[q] * tanh(self.cell_states[q])\n",
    "\n",
    "            # Compute output (before softmax)\n",
    "            outputs += [np.dot(self.wy, self.hidden_states[q]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # Backward Propagation Through Time\n",
    "    def backward(self, errors, inputs):\n",
    "        # Initialize gradients for all weights and biases\n",
    "        d_wf, d_bf = 0, 0\n",
    "        d_wi, d_bi = 0, 0\n",
    "        d_wc, d_bc = 0, 0\n",
    "        d_wo, d_bo = 0, 0\n",
    "        d_wy, d_by = 0, 0\n",
    "\n",
    "        # Initialize next hidden/cell state errors\n",
    "        dh_next, dc_next = np.zeros_like(self.hidden_states[0]), np.zeros_like(self.cell_states[0])\n",
    "\n",
    "        # Iterate backwards through sequence\n",
    "        for q in reversed(range(len(inputs))):\n",
    "            error = errors[q]\n",
    "\n",
    "            # Final output layer error\n",
    "            d_wy += np.dot(error, self.hidden_states[q].T)\n",
    "            d_by += error\n",
    "\n",
    "            # Error for hidden state\n",
    "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
    "\n",
    "            # Output gate gradient\n",
    "            d_o = tanh(self.cell_states[q]) * d_hs * sigmoid(self.output_gates[q], derivative=True)\n",
    "            d_wo += np.dot(d_o, inputs[q].T)\n",
    "            d_bo += d_o\n",
    "\n",
    "            # Cell state gradient\n",
    "            d_cs = tanh(tanh(self.cell_states[q]), derivative=True) * self.output_gates[q] * d_hs + dc_next\n",
    "\n",
    "            # Forget gate gradient\n",
    "            d_f = d_cs * self.cell_states[q - 1] * sigmoid(self.forget_gates[q], derivative=True)\n",
    "            d_wf += np.dot(d_f, inputs[q].T)\n",
    "            d_bf += d_f\n",
    "\n",
    "            # Input gate gradient\n",
    "            d_i = d_cs * self.candidate_gates[q] * sigmoid(self.input_gates[q], derivative=True)\n",
    "            d_wi += np.dot(d_i, inputs[q].T)\n",
    "            d_bi += d_i\n",
    "            \n",
    "            # Candidate gate gradient\n",
    "            d_c = d_cs * self.input_gates[q] * tanh(self.candidate_gates[q], derivative=True)\n",
    "            d_wc += np.dot(d_c, inputs[q].T)\n",
    "            d_bc += d_c\n",
    "\n",
    "            # Concatenated input error (sum of errors from all gates)\n",
    "            d_z = np.dot(self.wf.T, d_f) + np.dot(self.wi.T, d_i) + np.dot(self.wc.T, d_c) + np.dot(self.wo.T, d_o)\n",
    "\n",
    "            # Propagate error to next timestep\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.forget_gates[q] * d_cs\n",
    "\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
    "            np.clip(d_, -1, 1, out=d_)\n",
    "\n",
    "        # Gradient descent parameter updates\n",
    "        self.wf += d_wf * self.learning_rate\n",
    "        self.bf += d_bf * self.learning_rate\n",
    "\n",
    "        self.wi += d_wi * self.learning_rate\n",
    "        self.bi += d_bi * self.learning_rate\n",
    "\n",
    "        self.wc += d_wc * self.learning_rate\n",
    "        self.bc += d_bc * self.learning_rate\n",
    "\n",
    "        self.wo += d_wo * self.learning_rate\n",
    "        self.bo += d_bo * self.learning_rate\n",
    "\n",
    "        self.wy += d_wy * self.learning_rate\n",
    "        self.by += d_by * self.learning_rate\n",
    "\n",
    "    # Training function\n",
    "    def train(self, inputs, labels):\n",
    "        inputs = [oneHotEncode(input) for input in inputs]\n",
    "\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            predictions = self.forward(inputs)\n",
    "\n",
    "            errors = []\n",
    "            for q in range(len(predictions)):\n",
    "                # Cross-entropy error signal\n",
    "                errors += [-softmax(predictions[q])]\n",
    "                errors[-1][char_to_idx[labels[q]]] += 1\n",
    "\n",
    "            # Backpropagation through time\n",
    "            self.backward(errors, self.concat_inputs)\n",
    "    \n",
    "    # Testing function\n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([oneHotEncode(input) for input in inputs])\n",
    "\n",
    "        output = ''\n",
    "        for q in range(len(labels)):\n",
    "            # Sample prediction from softmax probabilities\n",
    "            prediction = idx_to_char[np.random.choice(\n",
    "                [*range(char_size)], p=softmax(probabilities[q].reshape(-1))\n",
    "            )]\n",
    "\n",
    "            output += prediction\n",
    "\n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "\n",
    "        print(f'Ground Truth:\\n\\t{labels}\\n')\n",
    "        print(f'Predictions:\\n\\t{\"\".join(output)}\\n')\n",
    "        print(f'Accuracy: {round(accuracy * 100 / len(inputs), 2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed693b59-a577-4392-88c3-17c942c38c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:47<00:00, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:\n",
      "\to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make\n",
      "\n",
      "Predictions:\n",
      "\to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his qusetus make\n",
      "\n",
      "Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Network\n",
    "hidden_size = 25\n",
    "\n",
    "lstm = LSTM(input_size = char_size + hidden_size, hidden_size = hidden_size, output_size = char_size, num_epochs = 1_000, learning_rate = 0.05)\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X, train_y)\n",
    "\n",
    "##### Testing #####\n",
    "lstm.test(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c32e5-0d97-4606-9fe1-eb65dc67bd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
